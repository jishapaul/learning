
Jupyter Notebook
Assignment 3 Last Checkpoint: 6 minutes ago (autosaved) [Python 3]

Python 3

    File
    Edit
    View
    Insert
    Cell
    Kernel
    Widgets
    Help

You are currently looking at version 1.1 of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the Jupyter Notebook FAQ course resource.
Assignment 3

In this assignment you will explore text message data and create models to predict if a message is spam or not.

import pandas as pd

import numpy as np

?

spam_data = pd.read_csv('spam.csv')

?

spam_data['target'] = np.where(spam_data['target']=='spam',1,0)

spam_data.head(10)

	text 	target
0 	Go until jurong point, crazy.. Available only ... 	0
1 	Ok lar... Joking wif u oni... 	0
2 	Free entry in 2 a wkly comp to win FA Cup fina... 	1
3 	U dun say so early hor... U c already then say... 	0
4 	Nah I don't think he goes to usf, he lives aro... 	0
5 	FreeMsg Hey there darling it's been 3 week's n... 	1
6 	Even my brother is not like to speak with me. ... 	0
7 	As per your request 'Melle Melle (Oru Minnamin... 	0
8 	WINNER!! As a valued network customer you have... 	1
9 	Had your mobile 11 months or more? U R entitle... 	1

from sklearn.model_selection import train_test_split

?

?

X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], 

                                                    spam_data['target'], 

                                                    random_state=0)

Question 1

What percentage of the documents in spam_data are spam?

This function should return a float, the percent value (i.e. ratio*100ratio*100).

def answer_one():

    print (spam_data.shape)

    arr = (spam_data['target'].value_counts())

    print (arr)

    res = float(arr[1])*100/(arr[0]+arr[1])

    print (res)

    return res

answer_one()

(5572, 2)
0    4825
1     747
Name: target, dtype: int64
13.4063173008

13.406317300789663

Question 2

Fit the training data X_train using a Count Vectorizer with default parameters.

What is the longest token in the vocabulary?

This function should return a string.

from sklearn.feature_extraction.text import CountVectorizer

?

def answer_two():

    vect = CountVectorizer().fit(X_train)

    k = vect.get_feature_names()

    

    df = pd.DataFrame(k,columns=['tokens'])

    

    df['len'] = df['tokens'].apply(lambda x: len(x))

    

    ln = max(df['len'])

    token_value =  df.loc[df['len'] ==ln, 'tokens'].values.tolist()[0]

 

       

    return token_value

answer_two()

'com1win150ppmx3age16subscription'

Question 3

Fit and transform the training data X_train using a Count Vectorizer with default parameters.

Next, fit a fit a multinomial Naive Bayes classifier model with smoothing alpha=0.1. Find the area under the curve (AUC) score using the transformed test data.

This function should return the AUC score as a float.

from sklearn.naive_bayes import MultinomialNB

from sklearn.metrics import roc_auc_score

?

def answer_three():

    vect = CountVectorizer().fit(X_train)

    X_train_vectorized = vect.transform(X_train)

    model = MultinomialNB(alpha=0.1)

    model.fit(X_train_vectorized, y_train)

    predictions = model.predict(vect.transform(X_test))

?

    print('AUC: ', roc_auc_score(y_test, predictions))

    

    return roc_auc_score(y_test, predictions)

answer_three()

AUC:  0.972081218274

0.97208121827411165

Question 4

Fit and transform the training data X_train using a Tfidf Vectorizer with default parameters.

What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?

Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series should be the feature name, and the data should be the tf-idf.

The series of 20 features with smallest tf-idfs should be sorted smallest tfidf first, the list of 20 features with largest tf-idfs should be sorted largest first.

This function should return a tuple of two series (smallest tf-idfs series, largest tf-idfs series).

from sklearn.feature_extraction.text import TfidfVectorizer

?

def answer_four():

    vect = TfidfVectorizer().fit(X_train)

    #print ((vect.get_feature_names()))

    

    feature_names = np.array(vect.get_feature_names())

    X_train_vectorized = vect.transform(X_train)

    #print (X_train_vectorized)

?

    sorted_tfidf_index_max = X_train_vectorized.max(0).toarray()[0].argsort()

    print (sorted_tfidf_index_max)

    sorted_tfidf_index_min = X_train_vectorized.max(0).toarray()[0].argsort()

    

    tfidf_values_max = X_train_vectorized.max(0).toarray()[0]

    tfidf_values_min = X_train_vectorized.min(0).toarray()[0]

?

    #print('Smallest tfidf:\n{}\n'.format(feature_names[sorted_tfidf_index[:20]]))

    #print('Largest tfidf: \n{}'.format(feature_names[sorted_tfidf_index[:-21:-1]]))

    

    smallest_tf_idfs_series = pd.Series(tfidf_values_min[sorted_tfidf_index_min[:20]],index =sorted(feature_names[sorted_tfidf_index_min[:20]]))

?

    largest_tf_idfs_series = pd.Series(tfidf_values_max[sorted_tfidf_index_max[:-21:-1]],index =sorted(feature_names[sorted_tfidf_index_max[:-21:-1]]))

    #print (smallest_tf_idfs_series)

    #print (largest_tf_idfs_series)

    

    smallest_tf_idfs_series.index.name = None

    largest_tf_idfs_series.index.name = None

    #smallest_tf_idfs_series.columns.name = None

    #largest_tf_idfs_series.columns.name = None

    

    return (smallest_tf_idfs_series,largest_tf_idfs_series)

answer_four()


Top 5 min tf-idf:
6561      to  0.023205
3558      it  0.027758
6446     the  0.028577
4564     not  0.031338
3548      is  0.031698

Top 5 max tf-idf:
7328        yup  1.000000
7089      where  1.000000
6598        too  1.000000
6439      thanx  1.000000
6434      thank  1.000000