
Assignment 2 - Introduction to NLTK

In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling.
Part 1 - Analyzing Moby Dick

import nltk

import pandas as pd

import numpy as np

from nltk.book import *

from nltk.metrics import *

    

​

# If you would like to work with the raw text you can use 'moby_raw'

with open('moby.txt', 'r') as f:

    moby_raw = f.read()

    

# If you would like to work with the novel in nltk.Text format you can use 'text1'

moby_tokens = nltk.word_tokenize(moby_raw)

text1 = nltk.Text(moby_tokens)

Example 1

How many tokens (words and punctuation symbols) are in text1?

This function should return an integer.

def example_one():

    

    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)

​

example_one()

Example 2

How many unique tokens (unique words and punctuation) does text1 have?

This function should return an integer.

def example_two():

    

    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))

​

example_two()

Example 3

After lemmatizing the verbs, how many unique tokens does text1 have?

This function should return an integer.

from nltk.stem import WordNetLemmatizer

​

def example_three():

​

    lemmatizer = WordNetLemmatizer()

    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]

​

    return len(set(lemmatized))

​

example_three()

Question 1

What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)

This function should return a float.

def answer_one():

    

    unique_tokens = len(set(nltk.word_tokenize(moby_raw)))

    total_token = len(nltk.word_tokenize(moby_raw))

    

    

    

    return float(unique_tokens)/total_token

​

answer_one()

Question 2

What percentage of tokens is 'whale'or 'Whale'?

This function should return a float.

def answer_two():

    

    dist = FreqDist(nltk.word_tokenize(moby_raw))

    c =  dist['whale'] +dist['Whale']

​

    

    return float(c)*100/len(nltk.word_tokenize(moby_raw))

​

answer_two()

Question 3

What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?

This function should return a list of 20 tuples where each tuple is of the form (token, frequency). The list should be sorted in descending order of frequency.

def answer_three():

    

    tokens = nltk.word_tokenize(moby_raw)

    

    dist = FreqDist(tokens)

    list = []

    

    for w in sorted(dist, key=dist.get, reverse=True):

        tup = (w, dist[w])

        list.append(tup)

        

​

    

    

    return list[:20]

​

answer_three()

Question 4

What tokens have a length of greater than 5 and frequency of more than 150?

This function should return a sorted list of the tokens that match the above constraints. To sort your list, use sorted()

def answer_four():

    tokens = nltk.word_tokenize(moby_raw)

    dist = FreqDist(tokens)

    list = [w for w in nltk.word_tokenize(moby_raw) if len(w) > 5 and dist[w]>150]

    

    return sorted(set(list))

​

answer_four()

Question 5

Find the longest word in text1 and that word's length.

This function should return a tuple (longest_word, length).

def answer_five():

    lst = (list(set(text1)))

    

   

    

    save = ""

    ln = 0

    for x in lst:

        if len(x)>ln:

            save = x

            ln = len(x)

            

    

    

    return (save,ln)

​

answer_five()

Question 6

What unique words have a frequency of more than 2000? What is their frequency?

"Hint: you may want to use isalpha() to check if the token is a word and not punctuation."

This function should return a list of tuples of the form (frequency, word) sorted in descending order of frequency.

def answer_six():

    tokens = nltk.word_tokenize(moby_raw)

    dist = FreqDist(tokens)    

    

    lst = []

    

    for w in sorted(dist, key=dist.get, reverse=True):

        if w.isalpha() and dist[w]>2000:

            tup = ( dist[w],w)

            lst.append(tup)

        

    return lst

​

answer_six()

Question 7

What is the average number of tokens per sentence?

This function should return a float.

def answer_seven():

    sentences = nltk.sent_tokenize(moby_raw)

    len_sent = len(sentences)

    ln = 0 

    

    for rows in sentences:

        ln = ln+len(nltk.word_tokenize(rows))

    

    result = float(ln)/9852

    

    return result

​

answer_seven()

Question 8

What are the 5 most frequent parts of speech in this text? What is their frequency?

This function should return a list of tuples of the form (part_of_speech, frequency) sorted in descending order of frequency.

def answer_eight():

                    

    txt = nltk.pos_tag((moby_tokens))

    result = nltk.FreqDist(tag for (word, tag) in txt).most_common(5)

    return result

​

answer_eight()

Part 2 - Spelling Recommender

For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.

For every misspelled word, the recommender should find find the word in correct_spellings that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.

*Each of the three different recommenders will use a different distance measure (outlined below).

Each of the recommenders should provide recommendations for the three default words provided: ['cormulent', 'incendenece', 'validrate'].

from nltk.corpus import words

​

correct_spellings = words.words()

Question 9

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

Jaccard distance on the trigrams of the two words.

This function should return a list of length three: ['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):

    from nltk.util import ngrams

      

    

    # store results

    result = []

    

    for i in range (0,3):

        min_distance = 1

        

        #Find words with same letter with the word in entries

        words_startswith = []

        for word in correct_spellings:

            if word.startswith(entries[i][0]):

                words_startswith.append(word)

                

                

        w = ""

        for word in words_startswith:

            # jaccard distance between trigram of word and trigram of word in entries

            j =  jaccard_distance(set(ngrams(word,3)), set(ngrams(entries[i],3)) )

            if j <min_distance:

                 #print (word)

                 w = word 

                 min_distance = j

        result.append(w)

        

    

   

    return result

    

answer_nine()

Question 10

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

Jaccard distance on the 4-grams of the two words.

This function should return a list of length three: ['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):

    from nltk.util import ngrams

      

    

    # store results

    result = []

    

    for i in range (0,3):

        min_distance = 1

        

        #Find words with same letter with the word in entries

        words_startswith = []

        for word in correct_spellings:

            if word.startswith(entries[i][0]):

                words_startswith.append(word)

                

                

        w = ""

        for word in words_startswith:

            # jaccard distance between trigram of word and trigram of word in entries

            j =  jaccard_distance(set(ngrams(word,4)), set(ngrams(entries[i],4)) )

            if j <min_distance:

                 w = word 

                 min_distance = j

        result.append(w)

        

    

   

    return result

    

answer_ten()

Question 11

For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:

Edit distance on the two words with transpositions.

This function should return a list of length three: ['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation'].

def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):

    

    result =[]

    

    for i in range (0,3):

        min_distance = 15

        w = ""

        for word in correct_spellings:

            if not word.startswith(entries[i][0]):

                continue

            distance = edit_distance(entries[i], word, substitution_cost=1, transpositions=True)

            if distance < min_distance:

                w = word

                min_distance = distance

        result.append(w)

                

                

    return result

    

answer_eleven()

​

